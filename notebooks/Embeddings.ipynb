{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the dimensionality of glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch as tt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94660d1be5f244dfb4956d01937a661c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2f9afa22914906b9a452541083c25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# most of the code in the cell is from Stackoverflow\n",
    "GLOVE_FILE = '../data/glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "# Get number of vectors and hidden dim\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "embedding_size = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, embedding_size), dtype=np.float32)\n",
    "embeds = {}\n",
    "\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "        embeds[line.split(' ')[0]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 10\n",
    "batch_size = 100000\n",
    "epochs = 1000\n",
    "learning_rate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(embedding_size, target_size)\n",
    "        self.decoder = nn.Linear(target_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1000], loss:2.0097\n",
      "epoch [11/1000], loss:1.6421\n",
      "epoch [21/1000], loss:1.4106\n",
      "epoch [31/1000], loss:1.2638\n",
      "epoch [41/1000], loss:1.1791\n",
      "epoch [51/1000], loss:1.1312\n",
      "epoch [61/1000], loss:1.1027\n",
      "epoch [71/1000], loss:1.0845\n",
      "epoch [81/1000], loss:1.0717\n",
      "epoch [91/1000], loss:1.0619\n",
      "epoch [101/1000], loss:1.0540\n",
      "epoch [111/1000], loss:1.0473\n",
      "epoch [121/1000], loss:1.0415\n",
      "epoch [131/1000], loss:1.0364\n",
      "epoch [141/1000], loss:1.0319\n",
      "epoch [151/1000], loss:1.0278\n",
      "epoch [161/1000], loss:1.0243\n",
      "epoch [171/1000], loss:1.0212\n",
      "epoch [181/1000], loss:1.0185\n",
      "epoch [191/1000], loss:1.0162\n",
      "epoch [201/1000], loss:1.0142\n",
      "epoch [211/1000], loss:1.0125\n",
      "epoch [221/1000], loss:1.0110\n",
      "epoch [231/1000], loss:1.0097\n",
      "epoch [241/1000], loss:1.0086\n",
      "epoch [251/1000], loss:1.0076\n",
      "epoch [261/1000], loss:1.0067\n",
      "epoch [271/1000], loss:1.0060\n",
      "epoch [281/1000], loss:1.0053\n",
      "epoch [291/1000], loss:1.0047\n",
      "epoch [301/1000], loss:1.0041\n",
      "epoch [311/1000], loss:1.0036\n",
      "epoch [321/1000], loss:1.0032\n",
      "epoch [331/1000], loss:1.0027\n",
      "epoch [341/1000], loss:1.0024\n",
      "epoch [351/1000], loss:1.0020\n",
      "epoch [361/1000], loss:1.0017\n",
      "epoch [371/1000], loss:1.0014\n",
      "epoch [381/1000], loss:1.0011\n",
      "epoch [391/1000], loss:1.0008\n",
      "epoch [401/1000], loss:1.0005\n",
      "epoch [411/1000], loss:1.0003\n",
      "epoch [421/1000], loss:1.0000\n",
      "epoch [431/1000], loss:0.9998\n",
      "epoch [441/1000], loss:0.9996\n",
      "epoch [451/1000], loss:0.9994\n",
      "epoch [461/1000], loss:0.9991\n",
      "epoch [471/1000], loss:0.9989\n",
      "epoch [481/1000], loss:0.9987\n",
      "epoch [491/1000], loss:0.9985\n",
      "epoch [501/1000], loss:0.9983\n",
      "epoch [511/1000], loss:0.9981\n",
      "epoch [521/1000], loss:0.9980\n",
      "epoch [531/1000], loss:0.9978\n",
      "epoch [541/1000], loss:0.9976\n",
      "epoch [551/1000], loss:0.9974\n",
      "epoch [561/1000], loss:0.9973\n",
      "epoch [571/1000], loss:0.9971\n",
      "epoch [581/1000], loss:0.9970\n",
      "epoch [591/1000], loss:0.9968\n",
      "epoch [601/1000], loss:0.9967\n",
      "epoch [611/1000], loss:0.9966\n",
      "epoch [621/1000], loss:0.9965\n",
      "epoch [631/1000], loss:0.9964\n",
      "epoch [641/1000], loss:0.9963\n",
      "epoch [651/1000], loss:0.9962\n",
      "epoch [661/1000], loss:0.9961\n",
      "epoch [671/1000], loss:0.9960\n",
      "epoch [681/1000], loss:0.9960\n",
      "epoch [691/1000], loss:0.9959\n",
      "epoch [701/1000], loss:0.9958\n",
      "epoch [711/1000], loss:0.9958\n",
      "epoch [721/1000], loss:0.9957\n",
      "epoch [731/1000], loss:0.9957\n",
      "epoch [741/1000], loss:0.9956\n",
      "epoch [751/1000], loss:0.9956\n",
      "epoch [761/1000], loss:0.9956\n",
      "epoch [771/1000], loss:0.9955\n",
      "epoch [781/1000], loss:0.9955\n",
      "epoch [791/1000], loss:0.9955\n",
      "epoch [801/1000], loss:0.9954\n",
      "epoch [811/1000], loss:0.9954\n",
      "epoch [821/1000], loss:0.9954\n",
      "epoch [831/1000], loss:0.9954\n",
      "epoch [841/1000], loss:0.9954\n",
      "epoch [851/1000], loss:0.9953\n",
      "epoch [861/1000], loss:0.9953\n",
      "epoch [871/1000], loss:0.9953\n",
      "epoch [881/1000], loss:0.9953\n",
      "epoch [891/1000], loss:0.9953\n",
      "epoch [901/1000], loss:0.9953\n",
      "epoch [911/1000], loss:0.9953\n",
      "epoch [921/1000], loss:0.9953\n",
      "epoch [931/1000], loss:0.9953\n",
      "epoch [941/1000], loss:0.9952\n",
      "epoch [951/1000], loss:0.9952\n",
      "epoch [961/1000], loss:0.9952\n",
      "epoch [971/1000], loss:0.9952\n",
      "epoch [981/1000], loss:0.9952\n",
      "epoch [991/1000], loss:0.9952\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = tt.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    id = 0\n",
    "    while id < len(vecs):\n",
    "        if id + batch_size <=len(vecs):\n",
    "            batch = tt.tensor(vecs[id:id+batch_size])\n",
    "        else:\n",
    "            batch = tt.tensor(vecs[id:])\n",
    "        id += batch_size\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, epochs, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check outpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.679304 21.647974 3.03133\n",
      "[ 0.418       0.24968    -0.41242     0.1217      0.34527    -0.044457\n",
      " -0.49688    -0.17862    -0.00066023 -0.6566    ]\n",
      "[ 0.5069263   0.16810846 -0.6155796   0.35290584  0.31652027  0.09166866\n",
      " -0.46754444 -0.7058037  -0.22616246 -0.4175256 ]\n",
      "20.051197 15.998968 4.052229\n",
      "[ 0.013441  0.23682  -0.16899   0.40951   0.63812   0.47709  -0.42852\n",
      " -0.55641  -0.364    -0.23938 ]\n",
      "[ 0.25697047  0.20482409 -0.20199686  0.52915436  0.532064   -0.04176922\n",
      " -0.7155416  -0.63826287 -0.49456692 -0.32819343]\n",
      "19.77369 16.451874 3.3218155\n",
      "[ 0.15164  0.30177 -0.16763  0.17684  0.31719  0.33973 -0.43478 -0.31086\n",
      " -0.44999 -0.29486]\n",
      "[ 0.35975456  0.25510624 -0.2952394   0.3254284   0.39722192  0.0540567\n",
      " -0.6364787  -0.75441164 -0.37787542 -0.31099516]\n",
      "24.562908 19.59288 4.970028\n",
      "[ 0.70853  0.57088 -0.4716   0.18048  0.54449  0.72603  0.18157 -0.52393\n",
      "  0.10381 -0.17566]\n",
      "[ 0.5962781  -0.00294097 -0.6001053   0.35469854  0.39668727  0.25445452\n",
      " -0.4427682  -0.5820744  -0.30185026 -0.31121904]\n",
      "24.570566 19.05914 5.511427\n",
      "[ 0.68047  -0.039263  0.30186  -0.17792   0.42962   0.032246 -0.41376\n",
      "  0.13228  -0.29847  -0.085253]\n",
      "[ 0.38896754  0.23852983 -0.15917706  0.2576198   0.5080813   0.14177218\n",
      " -0.62628514 -0.79971325 -0.46423492 -0.12100941]\n",
      "21.768545 16.456846 5.311699\n",
      "[ 0.26818   0.14346  -0.27877   0.016257  0.11384   0.69923  -0.51332\n",
      " -0.47368  -0.33075  -0.13834 ]\n",
      "[ 0.5124657   0.0220651  -0.40916348  0.23805258  0.52360994  0.42204696\n",
      " -0.6662147  -0.6571039  -0.30633873 -0.17885697]\n",
      "24.87188 21.029346 3.842533\n",
      "[ 0.33042   0.24995  -0.60874   0.10923   0.036372  0.151    -0.55083\n",
      " -0.074239 -0.092307 -0.32821 ]\n",
      "[ 0.48681653  0.2258426  -0.51125956  0.41892606  0.29317367 -0.01416375\n",
      " -0.47140762 -0.45266587 -0.29626688 -0.35245514]\n",
      "28.088675 20.088657 8.000017\n",
      "[ 0.21705  0.46515 -0.46757  0.10082  1.0135   0.74845 -0.53104 -0.26256\n",
      "  0.16812  0.13182]\n",
      "[ 0.35290748  0.26700082 -0.3623647   0.2772891   0.47650343  0.21172974\n",
      " -0.7287122  -0.6433586  -0.24466343 -0.11020479]\n",
      "34.234097 25.592056 8.64204\n",
      "[ 0.25769   0.45629  -0.76974  -0.37679   0.59272  -0.063527  0.20545\n",
      " -0.57385  -0.29009  -0.13662 ]\n",
      "[ 0.6045195   0.31436273 -0.86449015  0.33700547 -0.21843332 -0.4095318\n",
      " -0.4044258  -1.1272497  -0.26358223 -0.42116815]\n",
      "25.163013 19.375353 5.7876606\n",
      "[ 0.23727  0.40478 -0.20547  0.58805  0.65533  0.32867 -0.81964 -0.23236\n",
      "  0.27428  0.24265]\n",
      "[ 0.53535676  0.18368983 -0.40455425  0.29668802  0.4857142   0.27085242\n",
      " -0.7648083  -0.6962151  -0.3524652  -0.33011144]\n"
     ]
    }
   ],
   "source": [
    "for vec in vecs[:10]:\n",
    "    prediction = model(tt.tensor(vec)).detach().numpy()\n",
    "    difference = np.sum(vec**2)- np.sum(prediction**2)\n",
    "    print(np.sum(vec**2), np.sum(prediction ** 2), difference)\n",
    "    print(vec[:10])\n",
    "    print(prediction[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57da231913834bb6af657ce4d0b12665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# most of the code in the cell is from Stackoverflow\n",
    "NEW_GLOVE_FILE = '../data/glove.6B/glove.6B.' + str(target_size) + 'd.txt'\n",
    "embed_inverse = {embeds[string]:string for string in embeds.keys()}\n",
    "\n",
    "with open(NEW_GLOVE_FILE, 'w') as file:\n",
    "    for i in tqdm(range(len(vecs))):\n",
    "        file.write(embed_inverse[i] + \" \")\n",
    "        new_embedding = model.encoder(tt.tensor(vecs[i])).detach().numpy()\n",
    "        file.write(\" \".join(str(x) for x in new_embedding) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
