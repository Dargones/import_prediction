{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This module provide functionality for converting parsed data into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/af9562/.conda/envs/import/lib/python3.8/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import joblib\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch as tt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be data produced by Filtering.ipynb\n",
    "df = pd.read_json('../data/all_data_new.json', lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some statistics about the number of files per repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b78dff81e4423c9dbd3f443fa1b79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2106230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "repo_sizes = {}\n",
    "for x in tqdm(df.repo.values):\n",
    "    repo_sizes[x] = repo_sizes.get(x, 0) + 1\n",
    "sizes = np.array([x for x in repo_sizes.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 300  # maximum repository size to display on the plot\n",
    "plt.hist(sizes, bins=35, range=(0, threshold))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 33, mean: 82.723774, less than 300: 0.952948\n"
     ]
    }
   ],
   "source": [
    "print(\"median: %d, mean: %f, less than %d: %f\" \n",
    "      %(np.median(sizes), np.mean(sizes), threshold, np.count_nonzero(sizes < threshold) / len(sizes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redicilously large repositories from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_repos = {x for x in repo_sizes.keys() if repo_sizes[x] > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repos: 25461, total files: 2106230, repos to be removed: 1192\n",
      "Total repos: 24269, total files: 1283271\n"
     ]
    }
   ],
   "source": [
    "print(\"Total repos: %d, total files: %d, repos to be removed: %d\" \n",
    "      %(len(df.repo.unique()), len(df), len(large_repos)))\n",
    "df = df[~df.repo.isin(large_repos)]\n",
    "print(\"Total repos: %d, total files: %d\" %(len(df.repo.unique()), len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word embeddings and defining functions to create classname embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dadc3b982948d3978ba233ed53873e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f578259057c748edabbfea6ce7cbaf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# most of the code in the cell is from Stackoverflow\n",
    "GLOVE_FILE = '../data/glove.6B/glove.6B.' + str(EMBEDDING_SIZE) + 'd.txt'\n",
    "\n",
    "# Get number of vectors and hidden dim\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "assert(hidden_dim==EMBEDDING_SIZE)\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "embeds = {}\n",
    "\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "        embeds[line.split(' ')[0]] = i\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(identifier):\n",
    "    \"copied from stackoverflow\"\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_stats = {\"total\": 0, \"unsuccess\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(classname):\n",
    "    \"\"\"\n",
    "    Get a classname and create an embedding for that classname as the average of embedding for words that\n",
    "    are parts of that classname.\n",
    "    I.e. embedding for \"ClassName\" would be the average of word embeddings for \"Class\" and \"Name\"\n",
    "    \"\"\"\n",
    "    parts = camel_case_split(classname)\n",
    "    embedding = np.zeros(EMBEDDING_SIZE, dtype=float)\n",
    "    embed_stats[\"total\"] += len(parts)\n",
    "    for part in parts:\n",
    "        if part.lower() in embeds:\n",
    "            embedding[:EMBEDDING_SIZE] += vecs[embeds[part.lower()]]\n",
    "        else:\n",
    "            embedding[:EMBEDDING_SIZE] += average_vec\n",
    "            embed_stats[\"unsuccess\"] += 1\n",
    "    embedding /= len(parts)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the names of all extended\\implemented classes to their fully-qualified names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(x, classes):\n",
    "    \"\"\"Get a classname, which might be a fully qualified classname or not and return the fully qualified classname,\n",
    "    for that class if such class is in the classes dictionary,\n",
    "    :param x:  the classname in question,\n",
    "    :param classes: a dictionary of the form classname -> list of packages in which this classname appears\n",
    "    \"\"\"\n",
    "    if x in classes:\n",
    "        return classes[x][0] + '.' + x\n",
    "    if x.split('.') == 1 or x.split('.')[-1] not in classes:\n",
    "        return \"\"\n",
    "    curr_package = '.'.join(x.split('.')[:-1])\n",
    "    curr_name = x.split('.')[-1]\n",
    "    for package in classes[curr_name]:\n",
    "        if curr_package in package:\n",
    "            return package + '.' + curr_name\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(data):\n",
    "    \"\"\"\n",
    "    Get a pandas table representing a repository (each row is a file) and,\n",
    "    replace all extended\\implemented classnames in each compilation unit in the dataset with the,\n",
    "    fully qualified classnames\n",
    "    \"\"\"\n",
    "    classes = {}\n",
    "    for i in range(len(data)):\n",
    "        if data.name.values[i] not in classes:\n",
    "            classes[data.name.values[i]]= []\n",
    "        classes[data.name.values[i]].append(data.package.values[i])\n",
    "    for i in range(len(data)):\n",
    "        data.extends.values[i] = change_name(data.extends.values[i], classes)\n",
    "        data.implements.values[i] = [change_name(x, classes) for x in data.implements.values[i]]\n",
    "        data.implements.values[i] = [x for x in data.implements.values[i] if x != \"\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd049b35e7874daf9674ebefd2e00145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run change_names on all repositories in the graph\n",
    "first = 0\n",
    "\n",
    "for i in tqdm(range(first, len(df))):\n",
    "    if first == i:\n",
    "        curr_repo = df.repo.values[first]\n",
    "    if (i == len(df) - 1) or (df.repo.values[i+1] != curr_repo):\n",
    "        df[first:i+1] = change_names(df[first:i+1])\n",
    "        first = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting repositories to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nodes(data, annotations, strings):\n",
    "    \"\"\"\n",
    "    Get the data about a repository in the pandas table format (a row is a file) and return the\n",
    "    dictionary that maps each fully qualified name in the repository to an index to be used in graph\n",
    "    construction. Also fill in the node annotations (node embeddings)\n",
    "    :param data:        several rows from the pandas dataframe returned by Filtering.ipynb with all files for particular repo\n",
    "    :param annotations: a dictionary to which to add node annotations\n",
    "    :param string:      a list to fill with fully qualified names. strings[i] should corespond to the fully-qualified\n",
    "                        name of node i\n",
    "    :return:            the dictionary that maps a fully qualified name to an index (the inverse of strings)\n",
    "    \"\"\"\n",
    "    ind = 0\n",
    "    string_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        full_name = data.package.values[i] + '.' + data.name.values[i]\n",
    "        if full_name in strings:\n",
    "            print(data.name.values[i], full_name, data.repo.values[0])\n",
    "            assert(full_name not in strings)\n",
    "        strings.append(full_name)\n",
    "        annotations.append(np.zeros(EMBEDDING_SIZE * 2, dtype=float))\n",
    "        annotations[-1][:EMBEDDING_SIZE] = get_embedding(data.name.values[i])\n",
    "        string_dict[full_name] = ind\n",
    "        ind += 1\n",
    "    return string_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_edges(data, string_dict, edges, stats, annotations):\n",
    "    \"\"\"\n",
    "    Initialize edges in the graph\n",
    "    TODO: this function is way to long, break it up\n",
    "    :param data:        several rows from the pandas dataframe returned by Filtering.ipynb with all files for particular repo\n",
    "    :param string_dict: the dictionary that maps a fully qualified name to an index\n",
    "    :param edges:       a list to fill with edges\n",
    "    :param stats:       for each edge of type 1 (simple imoprt), the number of alternative imports that could be added to a node\n",
    "    \"                   (essentially the number of negatves that could be proposed to a network)\n",
    "    :param annotations: a dictionary to which to add node annotations\n",
    "    \"\"\"\n",
    "    classes = {}\n",
    "    for i in range(len(data)):\n",
    "        if data.name.values[i] not in classes:\n",
    "            classes[data.name.values[i]]= []\n",
    "    classes[data.name.values[i]].append(data.package.values[i])\n",
    "        \n",
    "    for i in range(len(data)): # for each compilation unit in the repository\n",
    "        poss_imports = len(data) - 1  # number of CUs that could have been imported but were not\n",
    "        edges_added = 0  # number of imported CUs\n",
    "        from_name = data.package.values[i] + '.' + data.name.values[i] # the name of the given CUs\n",
    "        from_ind = string_dict[from_name]  # its index\n",
    "        classImports = set(data.classImports.values[i])\n",
    "        for j in range(len(data)):\n",
    "            to_name = data.package.values[j] + '.' + data.name.values[j] # the name of some othe CU\n",
    "            to_ind = string_dict[to_name]\n",
    "            if to_ind == from_ind:\n",
    "                continue\n",
    "            if data.package.values[i] == data.package.values[j]: # the same package -> import not possible\n",
    "                poss_imports -= 1\n",
    "                edges.append((from_ind, 2, to_ind))\n",
    "            # either pacakge level import or class level import:\n",
    "            elif data.package.values[j] in data.packageImports.values[i] or to_name in classImports:\n",
    "                poss_imports -= 1\n",
    "                if to_name == data.extends.values[i] or to_name in data.implements.values[i]:  # extends/implements\n",
    "                    edges.append((from_ind, 3, to_ind))\n",
    "                else:\n",
    "                    edges_added += 1\n",
    "                    edges.append((from_ind, 1, to_ind))\n",
    "        stats += [poss_imports] * edges_added\n",
    "        \n",
    "        # updating annotations with information about external imports\n",
    "        external_imports = 0\n",
    "        e_imports_dict = {\"\", }\n",
    "        for e_import in data.classImports.values[i]:\n",
    "            if change_name(e_import, classes) == \"\" and e_import not in e_imports_dict:\n",
    "                e_imports_dict.add(e_import)\n",
    "                external_imports += 1\n",
    "                annotations[from_ind][EMBEDDING_SIZE:] += get_embedding(e_import.split('.')[-1])\n",
    "        if external_imports > 0:\n",
    "            annotations[from_ind][EMBEDDING_SIZE:] /= external_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repo_to_graph(data, init_nodes, init_edges):\n",
    "    \"\"\"\n",
    "    Convert a repository to a graph such that can be easily stored \n",
    "    :param data: a fraction of df dataframe that contains information about the given repository\n",
    "    :param init_nodes: a function to initialize the nodes\n",
    "    :param init_edges: a function to initialize the edges\n",
    "    :return:     a dictionary of the form \"edges\":[], \"annotations\":[], \"targets\": []\n",
    "    \"\"\"\n",
    "    graph = {\"edges\":[], \"annotations\":[], \"strings\":[], \"stats\":[]}\n",
    "    string_dict = init_nodes(data, graph[\"annotations\"], graph[\"strings\"])\n",
    "    init_edges(data, string_dict, graph[\"edges\"], graph[\"stats\"], graph[\"annotations\"])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run repo_to_graph on all repositories\n",
    "first = 0\n",
    "graphs = []\n",
    "\n",
    "for i in tqdm(range(first, len(df))):\n",
    "    if first == i:\n",
    "        curr_repo = df.repo.values[first]\n",
    "    if (i == len(df) - 1) or (df.repo.values[i+1] != curr_repo):\n",
    "        graph = repo_to_graph(df[first:i+1], init_nodes, init_edges)\n",
    "        graph[\"repo\"] = curr_repo\n",
    "        graphs.append(graph)\n",
    "        first = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(graphs, \"../data/allgraphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing an example of a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.load(graphs, \"../data/allgraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of some particular repository ro display a graph of\n",
    "# repo = \"0x277F/mopm\"\n",
    "repo = \"example\"\n",
    "repo_ind = 0\n",
    "while repo != graphs[repo_ind][\"repo\"]:\n",
    "    repo_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graphs[repo_ind]\n",
    "# graph = test[1]  # comment this out\n",
    "G = nx.DiGraph()\n",
    "G_extends = nx.DiGraph()\n",
    "G_package = nx.Graph()\n",
    "for i in range(len(graph[\"strings\"])):\n",
    "    node = graph[\"strings\"][i]\n",
    "    # node = i  # comment this out\n",
    "    G.add_node(node)\n",
    "    G_package.add_node(node)\n",
    "    G_extends.add_node(node)\n",
    "for edge in graph[\"edges\"]:\n",
    "    node_from = graph[\"strings\"][edge[0]]\n",
    "    node_to = graph[\"strings\"][edge[2]]\n",
    "    # node_from, _, node_to = edge  # comment this out\n",
    "    if edge[1] == 1:\n",
    "        G.add_edge(node_from, node_to)\n",
    "    elif edge[1] == 2:\n",
    "        G_package.add_edge(node_from, node_to)\n",
    "    else:\n",
    "        G_extends.add_edge(node_from, node_to)\n",
    "nodelist = [x.split('.')[-1] for x in graph[\"strings\"]]\n",
    "pos=nx.spring_layout(G, k=0.7,iterations=100)\n",
    "plt.figure(figsize = (15,15))\n",
    "nx.draw_networkx_nodes(G,pos,\n",
    "               nodelist=G.nodes,\n",
    "               node_color='#B1A164',\n",
    "               node_size=300,\n",
    "               alpha=0.7)\n",
    "nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "nx.draw_networkx_edges(G,pos,alpha=1, width=2, arrows=True)\n",
    "nx.draw_networkx_edges(G_package,pos,alpha=0.1, width=2, arrows=True)\n",
    "nx.draw_networkx_edges(G_extends,pos,alpha=1, width=2, arrows=True, edge_color=\"b\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(graph[\"repo\"], graph[\"stats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining which graphs can be used for prediction purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_graphs(min_options):\n",
    "    \"\"\"\n",
    "    Count the number of graphs that conform with the following criterion: \n",
    "    there is at least one node A that imports class B such that \n",
    "    class A could potentially import at least min_options additional classes\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(graphs)):\n",
    "        for x in graphs[i][\"stats\"]:\n",
    "            if x >= min_options:\n",
    "                count += 1\n",
    "                break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total graphs: 24269\n",
      " 1 option: 21670\n",
      " 5 options: 20914\n",
      " 25 options: 12452\n",
      " 125 options: 2478\n"
     ]
    }
   ],
   "source": [
    "print(\"Total graphs: %d\\n 1 option: %d\\n 5 options: %d\\n 25 options: %d\\n 125 options: %d\" \n",
    "     %(len(graphs), count_graphs(1), count_graphs(4), count_graphs(24), count_graphs(124)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21670"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs = [graph for graph in graphs if sum(graph[\"stats\"]) > 0]\n",
    "len(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(graphs, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19503, 2167)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Targets for testing. Each target has an origin node (to which the import is made), the destination node (the node corresponding to imported class) and several negatives (e.g. nodes that could have been imported but are not). The network/baseline must determine which  one of this is the actual import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(graph, n_options):\n",
    "    possible_edge_ids = np.where(np.array(graph[\"stats\"]) >= n_options)[0]\n",
    "    if len(possible_edge_ids) == 0:\n",
    "        return []\n",
    "    edge_id = np.random.choice(possible_edge_ids)\n",
    "    from_id, _, to_id = [x for x in graph[\"edges\"] if x[1] == 1][edge_id]\n",
    "    connected = {x[2] for x in graph[\"edges\"] if x[0] == from_id}\n",
    "    connected.add(from_id)\n",
    "    all_options = [i for i in range(len(graph[\"annotations\"])) if i not in connected]\n",
    "    selected_options = np.random.choice(np.array(all_options), n_options, replace=False)\n",
    "    return [from_id, to_id] + list(selected_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debf70befd6044c89172c4b88a096cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2167.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for graph in tqdm(test):\n",
    "    for n_options in (1, 4, 24, 124):\n",
    "        graph[\"targets_\" + str(n_options)] = create_target(graph, n_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4a6b64a0a041e4a3c10193bf049f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19503.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for graph in tqdm(train):\n",
    "    for n_options in (1, 4, 24, 124):\n",
    "        graph[\"targets_\" + str(n_options)] = create_target(graph, n_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/graphsTrain50']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test, \"../data/graphsTest50\")\n",
    "joblib.dump(train, \"../data/graphsTrain50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = joblib.load(\"../data/graphsTest50\")\n",
    "train = joblib.load(\"../data/graphsTrain50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing embedding size and Saving as Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToPandas(dataset):\n",
    "    \"\"\"\n",
    "    Convert a given dataset to a pandas format that can be written directly to a json file\n",
    "    \"\"\"\n",
    "    datasetJson = {}\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        for key in dataset[i].keys():\n",
    "            if key not in datasetJson:\n",
    "                datasetJson[key] = []\n",
    "            datasetJson[key].append(dataset[i][key])\n",
    "    return pd.DataFrame(data=datasetJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5511f936ea7f4dbfab220e9dde89ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19503.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b748bd6bcfd64edeaa8db2975c0944b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2167.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainJson = convertToPandas(train)\n",
    "testJson = convertToPandas(test)\n",
    "validJson = trainJson[:(int)(len(trainJson)/25)]\n",
    "trainJson = trainJson[(int)(len(trainJson)/25):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 8  # target embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    \"\"\"Simple One layer Autoencoder\"\"\"\n",
    "    def __init__(self, target_size):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(EMBEDDING_SIZE, target_size)\n",
    "        self.decoder = nn.Linear(target_size, EMBEDDING_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleAutoencoder(\n",
       "  (encoder): Linear(in_features=50, out_features=8, bias=True)\n",
       "  (decoder): Linear(in_features=8, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleAutoencoder(target_size)\n",
    "model.load_state_dict(tt.load(\"../data/glove.6B/\" + str(target_size) + \".model\"))\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_size(x):\n",
    "    \"\"\"\n",
    "    Convert a two-part embedding from one size to another\n",
    "    \"\"\"\n",
    "    new_x = np.zeros((len(x), target_size * 2))\n",
    "    for i, vec in enumerate(x):\n",
    "        vec = tt.tensor(vec).double()\n",
    "        new_x[i][:target_size] = model.encoder(vec[:EMBEDDING_SIZE]).detach().numpy()\n",
    "        new_x[i][target_size:] = model.encoder(vec[EMBEDDING_SIZE:]).detach().numpy()\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d5d4f3f6874f48bd72818ed7c9d963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2167.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194d584ca32d4694843ee9e1269b6356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18723.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d29c1fe50784f6abf240a5d0b540d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=780.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testJson.annotations = testJson.annotations.progress_apply(change_size)\n",
    "trainJson.annotations = trainJson.annotations.progress_apply(change_size)\n",
    "validJson.annotations = validJson.annotations.progress_apply(change_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJson.to_json(\"../data/graphs/newMethod\"+str(target_size)+\"/train.json\", orient=\"records\")\n",
    "testJson.to_json(\"../data/graphs/newMethod\"+str(target_size)+\"/test.json\", orient=\"records\")\n",
    "validJson.to_json(\"../data/graphs/newMethod\"+str(target_size)+\"/valid.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "import",
   "language": "python",
   "name": "import"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
